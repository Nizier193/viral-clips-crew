{
    "clips": [
        {
            "rank": 1,
            "text": "What I want to leave you with is that it's both true that there's a ton of uncertainty and disagreement in the field of AI, and that companies are already building and deploying AI all over the place anyway, in ways that affect all of us. Left to their own devices, it looks like AI companies might go in a similar direction to social media companies, spending most of their resources on building web apps and fighting for users' attention. And by default, it looks like the enormous power of more advanced AI systems might stay concentrated in the hands of a small number of companies, or even a small number of individuals. But AI's potential goes so far beyond that. AI already lets us leap over language barriers and predict protein structures. More advanced systems could unlock clean, limitless fusion energy or revolutionize how we grow food or a thousand other things. And we each have a voice in what happens. We're not just data sources. We are users, we're workers, we're citizens. So, as tempting as it might be, we can't wait for clarity or expert consensus to figure out what we want to happen with AI. AI is already happening to us. What we can do is put policies in place to give us as clear a picture as we can get of how the technology is changing, and then we can get in the arena and push for futures we actually want. Thank you. APPLAUSE"
        },
        {
            "rank": 2,
            "text": "First, don't be intimidated, either by the technology itself or by the people and companies building it. On the technology, AI can be confusing, but it's not magical. There are some parts of AI systems we do already understand well, and even the parts we don't understand won't be opaque forever. An area of research known as AI interpretability has made quite a lot of progress in the last few years in making sense of what all those billions of numbers are doing. One team of researchers, for example, found a way to identify different parts of a neural network that they could dial up or dial down to make the AI's answers happier or angrier, more honest, more Machiavellian, and so on. If we can push forward this kind of research further, then five or 10 years from now, we might have a much clearer understanding of what's going on inside the so-called black box. And when it comes to those building the technology, technologists sometimes act as though if you're not elbows deep in the technical details, then you're not entitled to an opinion on what we should do with it. Expertise has its place, of course, but history shows us how important it is that the people affected by a new technology get to play a role in shaping how we use it, like the factory workers in the 20th century who fought for factory safety, or the disability advocates who made sure the World Wide Web was accessible. You don't have to be a scientist or engineer to have a voice. APPLAUSE"
        },
        {
            "rank": 3,
            "text": "One huge challenge in building artificial intelligence is that no one can agree on what it actually means to be intelligent. This is a strange place to be in when building a new tech. When the Wright brothers started experimenting with planes, they didn't know how to build one, but everyone knew what it meant to fly. With AI, on the other hand, different experts have completely different intuitions about what lies at the heart of intelligence. Is it problem-solving? Is it learning and adaptation? Are emotions or having a physical body somehow involved? We genuinely don't know, but different answers lead to radically different expectations about where the technology is going and how fast it'll get there. An example of how we're confused is how we used to talk about narrow versus general AI. For a long time, we talked in terms of two buckets. A lot of people thought we should just be dividing between narrow AI, trained for one specific task, like recommending the next YouTube video, versus artificial general intelligence, or AGI, that could do everything a human could do. We thought of this distinction, narrow versus general, as a core divide between what we could build and practice and what would actually be intelligent. But then, a year or two ago, along came ChatGPT. If you think about it, is it narrow AI trained for one specific task, or is it AGI and can do everything a human can do? Clearly, the answer is neither. It's certainly general purpose. It can code, write poetry, analyze business problems, help you fix your car, but it's a far cry from being able to do everything as well as you or I could do it. So it turns out this idea of generality doesn't actually seem to be the right dividing line between intelligent and not. And this kind of thing is a huge challenge for the whole field of AI right now. We don't have any agreement on what we're trying to build or on what the roadmap looks like from here."
        }
    ]
}